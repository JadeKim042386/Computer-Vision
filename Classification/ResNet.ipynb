{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## ResNet_Architecture"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![ResNet_Architecture](../images/ResNet_Architecture.PNG)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic Block"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 각 Convolution 직후와 Activation Function 전에 Batch Normalization adoption  \r\n",
    "  \r\n",
    "![BN](../images/ResNet_BN.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class BasicBlock(nn.Module):\r\n",
    "    expansion = 1\r\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\r\n",
    "        super(BasicBlock, self).__init__()\r\n",
    "\r\n",
    "        # BatchNorm에 bias가 포함되어 있으므로, conv2d는 bias=False로 설정\r\n",
    "        self.residual_function = nn.Sequential(\r\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\r\n",
    "            nn.BatchNorm2d(out_channels),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, stride=1, padding=1, bias=False),\r\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion),\r\n",
    "        )\r\n",
    "\r\n",
    "        # identity mapping, input과 output의 feature map size, filter 수가 동일한 경우 사용.\r\n",
    "        self.shortcut = nn.Sequential()\r\n",
    "\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "\r\n",
    "        # projection mapping using 1x1conv\r\n",
    "        # Addition 할 때 out_channels * BasicBlock.expansion와 같게 맞춰주기 위함\r\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\r\n",
    "            self.shortcut = nn.Sequential(\r\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\r\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\r\n",
    "            )\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.residual_function(x) + self.shortcut(x) # F(x) + x\r\n",
    "        x = self.relu(x) # ReLU(F(x) + x)\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BottleNeck"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class BottleNeck(nn.Module):\r\n",
    "    expansion = 4\r\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.residual_function = nn.Sequential(\r\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False), # 1x1 Conv\r\n",
    "            nn.BatchNorm2d(out_channels),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False), # 3x3 Conv\r\n",
    "            nn.BatchNorm2d(out_channels),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False), # 1x1 Conv\r\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\r\n",
    "        )\r\n",
    "\r\n",
    "        self.shortcut = nn.Sequential()\r\n",
    "\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "\r\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\r\n",
    "            self.shortcut = nn.Sequential(\r\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\r\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\r\n",
    "            )\r\n",
    "            \r\n",
    "    def forward(self, x):\r\n",
    "        x = self.residual_function(x) + self.shortcut(x)\r\n",
    "        x = self.relu(x)\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ResNet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ResNet(nn.Module):\r\n",
    "    def __init__(self, block, num_block, num_classes=10, init_weights=True):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.in_channels=64\r\n",
    "\r\n",
    "        self.conv1 = nn.Sequential(\r\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\r\n",
    "            nn.BatchNorm2d(64),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n",
    "        )\r\n",
    "\r\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\r\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\r\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\r\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\r\n",
    "\r\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\r\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\r\n",
    "\r\n",
    "        # weights inittialization\r\n",
    "        if init_weights:\r\n",
    "            self._initialize_weights()\r\n",
    "\r\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\r\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\r\n",
    "        layers = []\r\n",
    "        for stride in strides:\r\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\r\n",
    "            self.in_channels = out_channels * block.expansion\r\n",
    "\r\n",
    "        return nn.Sequential(*layers)\r\n",
    "\r\n",
    "    def forward(self,x):\r\n",
    "        output = self.conv1(x)\r\n",
    "        output = self.conv2_x(output)\r\n",
    "        x = self.conv3_x(output)\r\n",
    "        x = self.conv4_x(x)\r\n",
    "        x = self.conv5_x(x)\r\n",
    "        x = self.avg_pool(x)\r\n",
    "        x = x.view(x.size(0), -1)\r\n",
    "        x = self.fc(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "    # define weight initialization function\r\n",
    "    def _initialize_weights(self):\r\n",
    "        for m in self.modules():\r\n",
    "            if isinstance(m, nn.Conv2d):\r\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\r\n",
    "                if m.bias is not None:\r\n",
    "                    nn.init.constant_(m.bias, 0)\r\n",
    "            elif isinstance(m, nn.BatchNorm2d):\r\n",
    "                nn.init.constant_(m.weight, 1)\r\n",
    "                nn.init.constant_(m.bias, 0)\r\n",
    "            elif isinstance(m, nn.Linear):\r\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\r\n",
    "                nn.init.constant_(m.bias, 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Define"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def resnet18():\r\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\r\n",
    "\r\n",
    "def resnet34():\r\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\r\n",
    "\r\n",
    "def resnet50():\r\n",
    "    return ResNet(BottleNeck, [3,4,6,3])\r\n",
    "\r\n",
    "def resnet101():\r\n",
    "    return ResNet(BottleNeck, [3, 4, 23, 3])\r\n",
    "\r\n",
    "def resnet152():\r\n",
    "    return ResNet(BottleNeck, [3, 8, 36, 3])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "c18c2ce2eeff7956326c52dfc6c88194418eebb74e6de9b69ae4a4f626458f80"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}